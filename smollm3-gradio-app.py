"""
Gradio Interface for SmolLM3 (text) and SmolVLM2 (vision)
Compatible with Mac, Windows, and Linux with automatic CPU/GPU detection
Fixed version - ClearButton bug resolved
"""

import gradio as gr
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoProcessor, AutoModelForImageTextToText
from PIL import Image
import platform
import sys

# Configuration
DEVICE = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
print(f"ğŸ–¥ï¸ Device detected: {DEVICE}")
print(f"ğŸ’» System: {platform.system()} {platform.machine()}")
print(f"ğŸ Python: {sys.version.split()[0]}")

# Check versions
try:
    import transformers
    import gradio as gr_check
    print(f"ğŸ“¦ Transformers: {transformers.__version__}")
    print(f"ğŸ“¦ Gradio: {gr_check.__version__}")

    # Check minimum version
    trans_version = tuple(map(int, transformers.__version__.split('.')[:2]))
    grad_version = tuple(map(int, gr_check.__version__.split('.')[:2]))

    if trans_version < (4, 45):
        print("âš ï¸  WARNING: Transformers version too old!")
        print("   Run: pip install --upgrade transformers>=4.45.0")

    if grad_version < (4, 0):
        print("âš ï¸  WARNING: Gradio version too old!")
        print("   Run: pip install --upgrade gradio>=4.0.0")

except Exception as e:
    print(f"âš ï¸  Error during version check: {e}")

# Models to load
TEXT_MODEL = "HuggingFaceTB/SmolLM3-3B"  # Instruct version (no -Instruct in the name)
VISION_MODEL = "HuggingFaceTB/SmolVLM2-2.2B-Instruct"

# Alternative: use quantized versions if loading issues
# TEXT_MODEL = "ggml-org/SmolLM3-3B-GGUF"  # Quantized version
# VISION_MODEL = "HuggingFaceTB/SmolVLM2-2.2B-Instruct"

# Global variables for models
text_model = None
text_tokenizer = None
vision_model = None
vision_processor = None

def load_text_model():
    """Load SmolLM3 text model"""
    global text_model, text_tokenizer

    if text_model is None:
        print("ğŸ“¥ Loading SmolLM3-3B-Instruct...")
        try:
            text_tokenizer = AutoTokenizer.from_pretrained(
                TEXT_MODEL,
                trust_remote_code=True
            )

            # Optimized loading based on device
            if DEVICE == "cuda":
                try:
                    text_model = AutoModelForCausalLM.from_pretrained(
                        TEXT_MODEL,
                        dtype=torch.float16,
                        device_map="auto",
                        trust_remote_code=True
                    )
                except:
                    # Fallback without device_map if accelerate is not available
                    text_model = AutoModelForCausalLM.from_pretrained(
                        TEXT_MODEL,
                        dtype=torch.float16,
                        trust_remote_code=True
                    ).to(DEVICE)
            elif DEVICE == "mps":
                text_model = AutoModelForCausalLM.from_pretrained(
                    TEXT_MODEL,
                    dtype=torch.float16,
                    trust_remote_code=True
                ).to(DEVICE)
            else:
                text_model = AutoModelForCausalLM.from_pretrained(
                    TEXT_MODEL,
                    dtype=torch.float32,
                    trust_remote_code=True
                ).to(DEVICE)

            print("âœ… SmolLM3 loaded successfully!")
        except Exception as e:
            print(f"âŒ Error loading SmolLM3: {e}")
            raise

    return text_model, text_tokenizer

def load_vision_model():
    """Load SmolVLM2 vision model"""
    global vision_model, vision_processor

    if vision_model is None:
        print("ğŸ“¥ Loading SmolVLM2-2.2B-Instruct...")
        try:
            # Load processor with trust_remote_code
            vision_processor = AutoProcessor.from_pretrained(
                VISION_MODEL,
                trust_remote_code=True
            )

            # Optimized loading based on device
            if DEVICE == "cuda":
                try:
                    vision_model = AutoModelForImageTextToText.from_pretrained(
                        VISION_MODEL,
                        dtype=torch.float16,
                        device_map="auto",
                        trust_remote_code=True
                    )
                except:
                    # Fallback without device_map if accelerate is not available
                    vision_model = AutoModelForImageTextToText.from_pretrained(
                        VISION_MODEL,
                        dtype=torch.float16,
                        trust_remote_code=True
                    ).to(DEVICE)
            elif DEVICE == "mps":
                vision_model = AutoModelForImageTextToText.from_pretrained(
                    VISION_MODEL,
                    dtype=torch.float16,
                    trust_remote_code=True
                ).to(DEVICE)
            else:
                vision_model = AutoModelForImageTextToText.from_pretrained(
                    VISION_MODEL,
                    dtype=torch.float32,
                    trust_remote_code=True
                ).to(DEVICE)

            print("âœ… SmolVLM2 loaded successfully!")
        except Exception as e:
            print(f"âŒ Error loading SmolVLM2: {e}")
            print("\nğŸ’¡ Make sure you have the correct version:")
            print("   pip install --upgrade transformers>=4.45.0")
            raise

    return vision_model, vision_processor

def generate_text(prompt, max_length=512, temperature=0.7, top_p=0.9):
    """Generate text with SmolLM3"""
    if not prompt or prompt.strip() == "":
        return "âš ï¸ Please enter a prompt."

    try:
        model, tokenizer = load_text_model()

        # Format prompt for instruct model
        messages = [{"role": "user", "content": prompt}]
        input_text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        # Tokenization
        inputs = tokenizer(input_text, return_tensors="pt").to(DEVICE)

        # Generation
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_length,
                temperature=temperature,
                top_p=top_p,
                do_sample=temperature > 0,
                pad_token_id=tokenizer.eos_token_id
            )

        # Decoding
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Extract only the assistant's response
        if "assistant" in generated_text.lower():
            parts = generated_text.lower().split("assistant")
            if len(parts) > 1:
                response = generated_text[generated_text.lower().rfind("assistant") + 9:].strip()
            else:
                response = generated_text
        else:
            response = generated_text.replace(input_text, "").strip()

        return response if response else generated_text

    except Exception as e:
        import traceback
        error_detail = traceback.format_exc()
        print(f"Full error:\n{error_detail}")
        return f"âŒ Error: {str(e)}\n\nCheck the console for more details."

def analyze_image(image, question, max_length=256):
    """Analyze an image with SmolVLM2"""
    try:
        if image is None:
            return "âš ï¸ Please upload an image."

        if not question or question.strip() == "":
            return "âš ï¸ Please ask a question about the image."

        model, processor = load_vision_model()

        # Prepare image
        if not isinstance(image, Image.Image):
            image = Image.fromarray(image).convert("RGB")
        else:
            image = image.convert("RGB")

        # Format prompt for SmolVLM2
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": question}
                ]
            }
        ]

        prompt = processor.apply_chat_template(messages, add_generation_prompt=True)

        # Process image and text
        inputs = processor(text=prompt, images=[image], return_tensors="pt")
        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}

        # Generation
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_length,
                do_sample=False
            )

        # Decoding
        generated_text = processor.batch_decode(
            outputs,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False
        )[0]

        # Extract response
        if "Assistant:" in generated_text:
            response = generated_text.split("Assistant:")[-1].strip()
        elif question in generated_text:
            response = generated_text.split(question)[-1].strip()
        else:
            response = generated_text

        return response if response else generated_text

    except Exception as e:
        import traceback
        error_detail = traceback.format_exc()
        print(f"Full error:\n{error_detail}")

        # Detailed error message for user
        error_msg = f"âŒ Error: {str(e)}\n\n"

        if "Unrecognized processing class" in str(e):
            error_msg += "ğŸ’¡ Solution: Update transformers:\n"
            error_msg += "   pip install --upgrade transformers>=4.45.0\n\n"

        error_msg += "Check the console for more details."
        return error_msg

# Interface Gradio
with gr.Blocks(title="SmolLM3 & SmolVLM2", theme=gr.themes.Soft()) as demo:
    
    gr.Markdown("""
    # ğŸ¤– SmolLM3 & SmolVLM2 - Multimodal Interface

    Complete interface for text analysis and image processing with **SmolLM3** and **SmolVLM2** models from HuggingFace.

    **Features:**
    - ğŸ’¬ Text Mode: Text generation with SmolLM3-3B (instruct version)
    - ğŸ‘ï¸ Vision Mode: Image analysis with SmolVLM2-2.2B-Instruct
    - âš¡ Compatible with CPU, CUDA GPU, and Apple Silicon (MPS)
    - ğŸŒ Multilingual support (EN, FR, ES, DE, IT, PT)
    """)
    
    with gr.Tabs():
        # Text Mode Tab
        with gr.Tab("ğŸ’¬ Text Mode"):
            gr.Markdown("### Text generation with SmolLM3")
            
            with gr.Row():
                with gr.Column():
                    text_input = gr.Textbox(
                        label="Your question or prompt",
                        placeholder="Example: Explain relativity in simple terms...",
                        lines=5
                    )
                    
                    with gr.Row():
                        text_max_length = gr.Slider(
                            minimum=50,
                            maximum=1024,
                            value=512,
                            step=50,
                            label="Max Length"
                        )
                        text_temperature = gr.Slider(
                            minimum=0.1,
                            maximum=2.0,
                            value=0.7,
                            step=0.1,
                            label="Temperature (creativity)"
                        )
                    
                    text_submit = gr.Button("ğŸš€ Generate", variant="primary")

                with gr.Column():
                    text_output = gr.Textbox(
                        label="Response",
                        lines=15,
                        interactive=False
                    )
            
            # Clear button after component definition
            with gr.Row():
                clear_text_btn = gr.Button("ğŸ—‘ï¸ Clear All")
                clear_text_btn.click(
                    fn=lambda: ("", ""),
                    inputs=[],
                    outputs=[text_input, text_output]
                )
            
            gr.Markdown("""
            **Prompt Examples:**
            - *"Write a short story about a robot discovering friendship"*
            - *"Explain how neural networks work"*
            - *"Code a Python function to calculate the Fibonacci sequence"*
            """)
            
            # Predefined examples
            gr.Examples(
                examples=[
                    ["Explain general relativity in simple terms", 300, 0.7],
                    ["Write a haiku about artificial intelligence", 100, 0.9],
                    ["What is the difference between Python and JavaScript?", 400, 0.5],
                ],
                inputs=[text_input, text_max_length, text_temperature]
            )
            
            text_submit.click(
                fn=generate_text,
                inputs=[text_input, text_max_length, text_temperature],
                outputs=text_output
            )
        
        # Vision Mode Tab
        with gr.Tab("ğŸ‘ï¸ Vision Mode"):
            gr.Markdown("### Image analysis with SmolVLM2")
            
            with gr.Row():
                with gr.Column():
                    vision_image = gr.Image(
                        type="pil",
                        label="Upload an image"
                    )
                    vision_question = gr.Textbox(
                        label="Your question about the image",
                        placeholder="Example: Describe this image in detail...",
                        lines=3
                    )
                    vision_max_length = gr.Slider(
                        minimum=50,
                        maximum=512,
                        value=256,
                        step=50,
                        label="Max response length"
                    )
                    vision_submit = gr.Button("ğŸ” Analyze", variant="primary")
                
                with gr.Column():
                    vision_output = gr.Textbox(
                        label="Analysis",
                        lines=15,
                        interactive=False
                    )

            # Clear button after component definition
            with gr.Row():
                clear_vision_btn = gr.Button("ğŸ—‘ï¸ Clear All")
                clear_vision_btn.click(
                    fn=lambda: (None, "", ""),
                    inputs=[],
                    outputs=[vision_image, vision_question, vision_output]
                )
            
            gr.Markdown("""
            **SmolVLM2 Capabilities:**
            - Detailed image descriptions
            - Visual question answering
            - OCR and text reading in images
            - Object counting
            - Document and chart analysis
            """)
            
            vision_submit.click(
                fn=analyze_image,
                inputs=[vision_image, vision_question, vision_max_length],
                outputs=vision_output
            )
    
    gr.Markdown(f"""
    ---
    **System Information:**
    - Device: `{DEVICE}`
    - Platform: `{platform.system()} {platform.machine()}`
    - Python: `{sys.version.split()[0]}`
    - Text Model: `{TEXT_MODEL}`
    - Vision Model: `{VISION_MODEL}`

    ğŸ’¡ **Note:** Models load automatically on first use of each mode.

    âš ï¸ **In case of error:** Make sure you have installed `transformers>=4.53.0` and `gradio>=4.0.0`
    """)

if __name__ == "__main__":
    print("\nğŸš€ Launching Gradio interface...")
    print(f"ğŸ“ Interface will be available at: http://127.0.0.1:7860")
    print("\n" + "="*60)

    demo.launch(
        server_name="127.0.0.1",
        server_port=7860,
        share=False,
        show_error=True
    )